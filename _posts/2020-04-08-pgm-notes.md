---
layout: post
title: Probabilistic Graphical Models
permalink: /notes/probablistic-graphical-models
excerpt: A way to represent thought with probability and graphs
---

# PGM

* Directed Graph

* Undirected Graph

* Factor graph

* Pairwise potential

## Variable Elimination

## Message Passing

$m_{42}(x_1)$ denotes the "message" sent from node 4 to node 2.


### Computing multiple marginals

In general, need to compute two messages per edge

Can compute all marginals, at all nodes, by combining
incoming messages from adjacent factors

Each message can be computed once, according to message update schedule

## (Pairwise) Sum-Product Belief Propagation

Sometimes called sum-product algorithm because it is a sum and then a product,
sum of the neighbors of the node for which we're computing marginals, and a product
of **those** neighbors.

Directed graphical models were being used as expert systems, which need to infer their
"beliefs" about the probabilities of various events.

In graphs with cycles, marginal probability estimates are only approximate.

Message from no

### Example: Pose Estimation

(c.f., Ramanan 2006)

Gradient orientation histogramms, color histograms for observed variables
State variable (white) is 2d position and orientation of that part. Position in teh plane
and orientation angle, both of which are discretized.

Pairwise factor is product of conditional of variable given its parent (unobserved given observed).

### Details

Definition: Message-passing protocol
Definition: Synchronous Parallel Schedule
Definition: Global Sequential Schedule

Correctness: For a graph, and any node $i$, taking that node
breaks the remainder of the graph into several disjoint subsets.
Use the distributive law for for node-induced subgraphs to decompose the marginals.

### A Tree-Structured Factor Graph

Not a pairwise graph. Sum over 

Break down messages to/from factors and variables.

ONly needed for factors of degree-3 or higher. In general, focus on the pairwise case. This one is less important. To generalize it to higher factors you just have to do a little indexing.

Get marginal distribution of product of 

\\(
\begin{equation}
x=2 \\\\
x = 2
\end{equation}
\\)

\[
\bar{m}_{sf}(x_s) \rightarrow \text{message from variable S to factor f, vector of K_s non-negative numbers}
m_{fs}(x_s) \rightarrow \text{message from factor to variable s, vector of K_s non-negative numbers}
\]

Pearl in 1980s focused on directed graphical models, which was complicated by directionality of edges and multiple parents.

Polytree---tree but some of the variables have more than one parent. Factor graph framework makes this a special case.
### Normalization Constants

In many applications, the normalization constant $Z$ is useful to compute

For any node $s$, an induction argument shows that the product of incoming messages is the unnormalized sum over other variables.

Thus by propagating messages to a single root node,

But underflow problems in computing margijnal, so normalize after each message update to avoid underflow. 

What is the normalization constant $Z$. IF message normalizers $Z_i$ are chosen so that each message sums to 1, $\log Z = \log Z_1 + \log Z_2 + \log Z_3 + \log Z_4$. 


